---
title: "Replication of Study 2 from Human heuristics for AI-generated language are flawed (PNAS) by Jakesch et al. 2023"
author: "Cid Decatur (cdecatur@stanford.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc_depth: 3
---

## Introduction

Generative language models mediate communicative processes with humans and the psychological mechanisms of AI-mediated communication (AI-MC). People use AI to autoc-omplete or craft entire messages for their digitally communicative practices. My research program revolves around understanding how humans construct the identity of themselves and others with media (AI is of growing interest). As identity becomes psychologically salient while crafting a self-descriptive message online, we know communicators are likelier to bend or break the truth about themselves. AI plagiarism of a dating profile bio is unique in that the content of the message may be factually correct, yet the credit for composition skill and cues given off from the AI-generated bio take advantage of pretenses. This study will verify the accuracy with which message receivers can identify AI-plagiarized biographies. Attempting to replicate this study will bring me up to speed on managing the analytic chain of research and allow me to get my hands dirty with collecting and analyzing crowd worker data for the first time. The methods of analysis and statistics computed for the study are also challenging to me but not outside of my potential capability for the next eight weeks.

### Justification 

The most interesting portion of the original paper's findings is the "truth default" finding replicated in the context of A.I. In this experiemnt, they saw that participants defaulted to thinking the text was human-written even when they were alerted to the possibility that this wasn't the case. 

The most interesting part of this replication will be the finding around whether participants select true or false more often.

### Stimuli and Procedure 

The stimuli for this project are not publically available but are retrievable from my advisor and co-author on the original project, Jeffrey Hancock. The project has a repository with all stimuli data, including procedures, available on OSF. The data they collected is not fully public. The supplemental materials that describe data-collection procedures for the six studies in the original paper are in this directory.

The original study recruited 1,000 gender-balanced participants on Prolific and sampled from 1,000 user-generated dating bios and 1,000 AI-generated dating bios. They then briefed the participants on their task to identify the bios generated by AI, which they completed in a digital format. Each participant graded 12 bios, 6 generated by humans and 6 generated by chatGPT. 

Regarding challenges, I’ve never run an experiment by myself from start to finish, nor have I been the sole analyst on a data set. Conducting the statistical analysis, managing the analytic pipeline, and keeping data organized by myself without the help of others will be enough of a challenge for me. If I’m completing the project with too much ease, I’ll find ways to produce a more complete and challenging procedure. 

### Links 

Repository: <https://github.com/psych251/jakesch2023>

Original paper: <https://github.com/psych251/jakesch2023/blob/main/original_paper/Jakesch%20et%20al.%2C%202023.pdf>

Supplemental material: <https://github.com/psych251/jakesch2023/blob/main/original_paper/pnas.2208839120.sapp.pdf>

## Methods

**From Jakesch 2023** "Experiment Design. The six experiments combined elements of a simplified Turing test (21) with a data labeling task. After providing informed consent, participants were introduced to the hospitality, dating, or professional scenario. They were told that they were browsing an online platform where some users had written their self-presentations while an AI system generated other self-presentations. Participants completed two comprehension checks and rated 16 self-presentations, half generated by a state-of-the-art AI language model. They were asked to evaluate whether each self-presentation was generated by AI on a five-point Likert scale from “definitely AI-generated” to “definitely human-written.” Mirroring truth default behaviors observed in deception research (34), participants marked the self-presentations as “likely human-written” or “definitely human-written” in 53.8% of cases. In the remaining 46.2% of cases, they showed suspicion and selected either “not sure,” “likely AI-generated,” or “definitely AI-generated.” To allow for concise analysis, we used these two roughly balanced groups to create a binary signal corresponding to participants’ suspicion that a self-presentation may not be human-written. A robustness check using the full scale as the primary outcome metric showed similar results. Halfway through the rating task, participants in the three main experiments were asked to explain their judgment in an open-ended response. Asking participants to explain their reasoning did not change the accuracy of their subsequent ratings (see SI Appendix for details). Following the rating task, participants provided demographic information and indicated their experience with computer programming and AI technologies. Participants were debriefed about their performance and the purpose of the study. The Cornell University Institutional Review Board approved the study protocols. We preregistered the final two validation experiments prior to data collection (https://aspredicted.org/bz7x7.pdf).
We performed the experiments in three social contexts to increase robustness and generalizability. In addition, minor variations across experiments explored auxiliary hypotheses. We used longer self-presentations in the dating and professional-context experiments to test whether the length of self-presentations limited participants' accuracy. To keep the three main experiments’ duration comparable, we reduced the number of rated self-presentations to 12 in these two experiments. To explore the effect of increased effort (23), we offered half of the participants in the dating context a bonus payment if they rated at least 75% of the self-presentations correctly. There was no difference in performance between the bonus and no-bonus groups. Finally, to test whether participants could learn to detect generated self-presentations if they received feedback (18), half of the participants in the professional context were told whether their choice was correct after every rating, again with no difference in outcomes. An overview of the experimental designs is included in SI Appendix."


**Modifications to procedure (differences from original study**

First and foremost, there is only one context (Airbnb hospitality profiles) included in this report. 

Each participant reads 12 self-descriptions, half AI-generated, and half-human-written, sampled from an  set of profile bios and paired, as is the case in the original experiment. For the purposes of this replication, we decided it would be best to offer the participants a binary signal choice "generated by AI" or "written by human" under each self-presentation text. There was no incentivization procedure (participants in some conditions could receive more pay if they scored higher in accuracy, much too expensive!). Finally, in some experiments, the full paper told one round of participants they were correct on every response, this was not conducted in this experiment. All modifications listed were applied in one of the 6 experiments conducted by the original authors. This paper features a pared-down version of the first experiment from the original paper. 

The authors note that they used 2,000 participants for the original experiment with the hospitality context. They note that a sample size of 1,000 is good enough for replication at end of PNAS report. Additionally, participants were not asked to reflect on their choices mid-procedure as they were in some conditions of Jakesch et al. (2023). 


### Power Analysis and Sample Size 

The authors do not report which analyses they conducted to statistically compare means in different experimental conditions. This, in combination with time and financial restraints, results in a sample size to to be 318 participants.


### Materials

The stimuli for all three contexts have been retrieved from the authors. There are 3,000 profiles, 50% AI-generated and 50% human written. I used embedded data and Qualtrics in qualtrics to create a survey with 6 questions programmed to pull an AI profile at random and 6 to pull a human profile at random. These questions were then randomized in the survey flow. 

### Procedure	

Participants were presented with a hospitality scenario. They were told they were browsing Airbnb and they were made aware that some profiles were generated using artificial intelligence. Their job was to assess the source of each message using the two radio buttons in the survey. AFter participants offered consent, they were then led through the 12 judgements. After they completed the task, they were redirected to prolific and awarded compensation. 

**For replication**

### Analysis Plan

Data cleaning rules: The only columns that should remain are participant index values, coded responses, and the stored profile_id to display which text the participants were displaed for data integrity. 

The means of accuracy scores and distributions between human/ai factor levels were analyzed. 

With accuracy data collected through Prolific/Qualtrics: 

1) Run python script to anonymize data stimuli and data

1a) delete any remaining columns that get in the way of restructuring data, delete any incomplete trials. Since survey is so short, this will likely not pose any issues. 

2) Report overall mean of accuracy 

3) Filter for each scenario's human-generated, ai-generated response ratings 

4) Run two-sample, two-tailed t-test

5) Run binomial test for overall mean accuracy comparison with chance 

6) Run a logistic regression comparing AI and Human conditions with participants as a random effect. 

#### Actual Sample

**Notes:** 

100% completion rate. 

318 participants surveyed.

**Demographics**
  

#### Differences from pre-data collection methods plan

None. 

## Results


### Data preparation

Data preparation following the analysis plan.

>Below is for the pilot data collection 

**Load Libraries**
```{r include=F}
library(dplyr)
library(tidyverse)
library(tidyr)
library(lme4)
library(knitr)
```

**Import Data** 
>Please note: I implemented the AI_QX_ID col to capture stimuli IDs after running the 3 participants for pilot b, hence NAs. Additionally, Ive re-runthe analyses with pilot_b put in a variable called pilota so I don't have to recode the whole thing. 

```{r echo=TRUE}

#putting pilot b data in the pilota variable for ease of run.
pilota <- read.csv("Psych 251 Final Data Collection_December 12, 2023_15.53.csv")

head(pilota)

```

### Data Tidying 


```{r echo=TRUE}
#Need to convert data from raw responses to accuracy (0/1 binary variable)

#AI cols
pilota$AI_Q1 <- ifelse(pilota$AI_Q1 == 2, 0, pilota$AI_Q1)

pilota$AI_Q2 <- ifelse(pilota$AI_Q2 == 2, 0, pilota$AI_Q2)

pilota$AI_Q3 <- ifelse(pilota$AI_Q3 == 1, 0, ifelse(pilota$AI_Q3 == 2, 1, pilota$AI_Q3))

pilota$AI_Q4 <- ifelse(pilota$AI_Q4 == 2, 0, pilota$AI_Q4)

pilota$AI_Q5 <- ifelse(pilota$AI_Q5 == 1, 0, ifelse(pilota$AI_Q5 == 2, 1, pilota$AI_Q5))

pilota$AI_Q6 <- ifelse(pilota$AI_Q6 == 1, 0, ifelse(pilota$AI_Q6 == 2, 1, pilota$AI_Q6))

#Human cols 

pilota$HU_Q1 <- ifelse(pilota$HU_Q1 == 2, 0, pilota$HU_Q1)

pilota$HU_Q2 <- ifelse(pilota$HU_Q2 == 2, 0, pilota$HU_Q2)

##NOTE: Weird qualtrics coding had AI selection as four
pilota$HU_Q3 <- ifelse(pilota$HU_Q3 == 4, 0, pilota$HU_Q3)

pilota$HU_Q4 <- ifelse(pilota$HU_Q4 == 1, 0, ifelse(pilota$HU_Q4 == 2, 1, pilota$HU_Q4))

pilota$HU_Q5 <- ifelse(pilota$HU_Q5 == 1, 0, ifelse(pilota$HU_Q5 == 2, 1, pilota$HU_Q5))

pilota$HU_Q6 <- ifelse(pilota$HU_Q6 == 1, 0, ifelse(pilota$HU_Q6 == 2, 1, pilota$HU_Q6))

#Now, we wrangle

#make df with only ai_qs in long form 
ai_cols <- grep("^AI_Q[1-6]$", names(pilota), value = TRUE)
pilota_ai <- pilota %>%
  select(all_of(ai_cols)) %>%
  pivot_longer(cols = all_of(ai_cols), names_to = "AI_Q", values_to = "AI_acc")

#make df with only hu_qs in long form 
hu_cols <- grep("^HU_Q[1-6]$", names(pilota), value = TRUE)
pilota_hu <- pilota %>%
  select(all_of(hu_cols)) %>%
  pivot_longer(cols = all_of(hu_cols), names_to = "HU_Q", values_to = "HU_acc")

# make a row for participant numbers (6 obs/participant)
participants <- data.frame(Participant = rep(1:nrow(pilota), each = 6))

 #Combine AI, HU, and participant data frames
pilota_long <- cbind(participants, pilota_ai, pilota_hu)

head(pilota_long)
```

###Visualization
```{r echo=TRUE}
#for some reason, the accuracy data is being stored as a factor so I changed it to numeric 
pilota_long$AI_acc <- as.numeric(pilota_long$AI_acc)
pilota_long$HU_acc <- as.numeric(pilota_long$HU_acc)


mean_accuracy <- pilota_long %>%
  summarize(Mean_AI_acc = mean(AI_acc), 
            Mean_HU_acc = mean(HU_acc))

# Gather the data into long format for plotting
mean_accuracy_long <- mean_accuracy %>%
  gather(key = "Condition", value = "Mean_Accuracy", Mean_AI_acc, Mean_HU_acc)

# Create a bar plot with error bars
ggplot(mean_accuracy_long, aes(x = Condition, y = Mean_Accuracy, fill = Condition)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8), width = 0.7) +
  scale_fill_brewer(palette = "Set1", labels = c('AI Accuracy', 'Human Accuracy')) +
  geom_errorbar(aes(ymin = Mean_Accuracy - sd(Mean_Accuracy) / sqrt(length(Mean_Accuracy)), 
                    ymax = Mean_Accuracy + sd(Mean_Accuracy) / sqrt(length(Mean_Accuracy))), 
                position = position_dodge(width = 0.8), width = 0.25) +
  labs(x = "Condition", y = "Mean Accuracy") +
  theme_light() +
  scale_x_discrete(labels = c("Mean_AI_acc" = "AI Accuracy", "Mean_HU_acc" = "Human Accuracy")) +
  geom_text(aes(label = round(Mean_Accuracy, 2), x = Condition), 
            position = position_dodge(width = 0.8), 
            vjust = -0.5, hjust = 2)

```
**Original Visualization**

> note, the original visualization does not spil the conditions into to. This represents the overall accuracy accross conditions. The overall accuracy replicated within a percent. (see first chunk in **Analysis**)

![Jakesch et al (2023) bar plot. See the leftmost plot for the Airbnb Run](original_graph.png)


### Binomial test 

```{r echo= TRUE}
# Calculate overall accuracy for both AI and HU conditions (compare raw means)
overall_accuracy <- mean(c(pilota_long$AI_acc, pilota_long$HU_acc))

# Print the overall accuracy
print(overall_accuracy)


# Calculate chance accuracy (50%)
chance_accuracy <- 0.5

# Compare overall accuracy to chance accuracy
accuracy_comparison <- data.frame(
  Measure = c("Overall Accuracy", "Chance Accuracy"),
  Accuracy = c(overall_accuracy, chance_accuracy)
)

# Print the comparison
print(accuracy_comparison)


#Binomial Test

# Observed accuracy
observed_accuracy <- mean(c(pilota_long$AI_acc, pilota_long$HU_acc))

# Expected (chance) accuracy
chance_accuracy <- 0.5

# Perform a one-sample binomial test
binomial_test <- binom.test(sum(c(pilota_long$AI_acc, pilota_long$HU_acc)), 
                            length(c(pilota_long$AI_acc, pilota_long$HU_acc)), 
                            p = chance_accuracy, alternative = "two.sided")

# Print the results of the binomial test
print(binomial_test)
```


###Logistic regression 
```{r echo=TRUE}
pilota_lr <- pilota %>%
  pivot_longer(cols = starts_with("AI_Q") | starts_with("HU_Q"),
               names_to = "Question",
               values_to = "Accuracy")

# Create a binary 'Condition' variable (0 for AI, 1 for HU)
pilota_lr$Condition <- ifelse(grepl("^AI_", pilota_lr$Question), 0, 1)

# Create a unique participant identifier column
pilota_lr$Participant <- rep(1:(nrow(pilota_lr)/12), each = 12)

# Add 'Participant' as a factor variable
pilota_lr$Participant <- as.factor(pilota_lr$Participant)

# Convert 'Accuracy' to a numeric variable if it's not already
pilota_lr$Accuracy <- as.numeric(pilota_lr$Accuracy)

#get rid of Q_ID rows for this analysis in pilota_lr
pilota_lr <- pilota_lr[!grepl("_ID", pilota_lr$Question), ]

# Fit a logistic regression model controlling for nested observations
logistic_model <- glmer(Accuracy ~ Condition + (1 | Participant), 
                        data = pilota_lr, family = binomial)

# Summary of the logistic regression model
summary(logistic_model)
```

###T-test 
```{r echo=T}

#for pilot b need to grepl out Q_ID rows so we can group_by: 

pilota_lr <- pilota_lr[!grepl("_ID", pilota_lr$Question), ]

#then get individual avgs 
individual_avg <- pilota_lr %>%
  group_by(Participant, Condition) %>%
  summarize(Avg_Accuracy = mean(Accuracy))

t_test_result <- t.test(Avg_Accuracy ~ Condition, data = individual_avg)

# Display the t-test result
t_test_result

```

## Discussion

### Summary of replication results

While overall accuracy was significantly different from chance, the "effect size" is very small (~3.4% from 50%). This accuracy was very close to the original reported overall accuracy finding; people perform at or around chance when attempting to distinguish AI from human generated text. With this supporting evidence of the original finding, we can interpret that it could be the heuristics referenced in the original experiment that lower human accuracy. In effect, participants would be just as successful if they flipped a coin while running through the procedure. 

####Overal accuracy

```{r echo=TRUE}
print(binomial_test)
```

####Run two-sample, two-tailed t-test

```{r echo=TRUE}
print(t_test_result)
```
From this t-test (cond 0 = AI, cond 1 = human), Participants were significantly (p = 2.2e-16, t = -9.044, CI [-0.173, -0.111]) more likely to correctly identify human-generated text over AI-generated text. This is an interesting result. Participants were around chance overall, but were more accurate judging human self-presentations. Theorists in deception-detection long cited "truth-default" (Levine, 2019) when results mirror this experiment. Truth-default says participants are more likely to go with the "null" of the situation (e.g. "There likely is no lie" or "this text is likely generated by a human"). In the case of AI detection, it's possible that participants were more accurate at judging human profiles because they simply selected "human" more often than not (if a participant selects "human" for all 12 answers, they would still be 50% correct in this experimental design). See exploratory analysis for more on default. 

####Run a binomial family generalized linear mixed model to compare accuracy accross conditions with participants as a rand

```{r echo=TRUE}
print(logistic_model)
```


####Exploratory analysis: test for human default in participants 

Like many of those who run deception-detection paradigms, I am interested to see whether participants were more likely to select one response over another. I will measure this using_______ 

WHEN FINISHED CHECK IF CONSISTENT WITH COMMENTARY
### Commentary

This replication is not particularly surprising. What I am interested in is the default to marking "human" in the experiment. Having looked at the stimuli myself and participated in the study multiple times, my intuition told me people would select AI more often, since they're being told there are some "moles" in the stimluli, participants would not want to be duped, and thus would guess AI more often. Perhaps this finding speaks to what participants think generateive AI is capable of... 


