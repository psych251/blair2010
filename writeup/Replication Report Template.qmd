---
title: "Replication of Study 2 from Human heuristics for AI-generated language are flawed (PNAS) by Jakesch et al. 2023"
author: "Cid Decatur (cdecatur@stanford.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc_depth: 3
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction

Generative language models mediate communicative processes with humans and the psychological mechanisms of AI-mediated communication (AI-MC). People use AI to autocomplete or craft entire messages for their digitally communicative practices. My research program revolves around understanding how humans construct the identity of themselves and others with media (AI is of growing interest). As identity becomes psychologically salient while crafting a message online, we know communicators are likelier to bend or break the truth about themselves. AI plagiarism of a dating profile bio is unique in that the content of the message may be factually correct, yet the credit for compositional skill and cues given off from the AI-generated bio take advantage of pretenses. This study will verify the accuracy with which message receivers can identify AI-plagiarized biographies. Attempting to replicate this study will bring me up to speed on managing the analytic chain of research and allow me to get my hands dirty with collecting and analyzing crowd worker data for the first time. The methods of analysis and statistics computed for the study are also challenging to me but not outside of my potential capability for the next eight weeks.

### Justification 

The most interesting portion of the original paper's findings is the "truth default" finding replicated in the context of A.I. In this experiemnt, they saw that participants defaulted to thinking the text was human-written even when they were alerted to the possibility that this wasn't the case. 

The most interesting part of this replication will be the finding around whether participants select true or false more often.

### Stimuli and Procedure 

The stimuli for this project are not publically available but are retrievable from my advisor and co-author on the original project, Jeffrey Hancock. The project has a repository with all data and procedures available on OSF. The supplemental materials that describe data-collection procedures for the six studies in the original paper are in this directory.

The procedure for this experiment is relatively straightforward, using the AI-generated messages provided by the original authors. If training and scraping a new set of stimuli becomes more appropriate, this would be outside of my current skillset but would be possible if I wanted to take the paper to publication. The original study recruited 1,000 gender-balanced participants on Prolific and sampled from 1,000 user-generated dating bios and 1,000 AI-generated dating bios. They then briefed the participants on their task to identify the bios generated by AI, which they completed in a digital format. 

Regarding challenges, I’ve never run an experiment by myself from start to finish, nor have I been the sole analyst on a dataset. Conducting the statistical analysis, managing the analytic pipeline, and keeping data organized by myself without the help of others will be enough of a challenge for me. If I’m completing the project with too much ease, I’ll find ways to produce a more complete and challenging procedure. 

### Links 

Repository: <https://github.com/psych251/jakesch2023>

Original paper: <https://github.com/psych251/jakesch2023/blob/main/original_paper/Jakesch%20et%20al.%2C%202023.pdf>

Supplemental material: <https://github.com/psych251/jakesch2023/blob/main/original_paper/pnas.2208839120.sapp.pdf>

## Methods

Materials and Methods
**From Jakesch 2023** "Experiment Design. The six experiments combined elements of a simplified Turing test (21) with a data labeling task. After providing informed consent, participants were introduced to the hospitality, dating, or professional scenario. They were told that they were browsing an online platform where some users had written their self-presentations while an AI system generated other self-presentations. Participants completed two comprehension checks and rated 16 self-presentations, half generated by a state-of-the-art AI language model. They were asked to evaluate whether each self-presentation was generated by AI on a five-point Likert scale from “definitely AI-generated” to “definitely human-written.” Mirroring truth default behaviors observed in deception research (34), participants marked the self-presentations as “likely human-written” or “definitely human-written” in 53.8% of cases. In the remaining 46.2% of cases, they showed suspicion and selected either “not sure,” “likely AI-generated,” or “definitely AI-generated.” To allow for concise analysis, we used these two roughly balanced groups to create a binary signal corresponding to participants’ suspicion that a self-presentation may not be human-written. A robustness check using the full scale as the primary outcome metric showed similar results. Halfway through the rating task, participants in the three main experiments were asked to explain their judgment in an open-ended response. Asking participants to explain their reasoning did not change the accuracy of their subsequent ratings (see SI Appendix for details). Following the rating task, participants provided demographic information and indicated their experience with computer programming and AI technologies. Participants were debriefed about their performance and the purpose of the study. The Cornell University Institutional Review Board approved the study protocols. We preregistered the final two validation experiments prior to data collection (https://aspredicted.org/bz7x7.pdf).
We performed the experiments in three social contexts to increase robustness and generalizability. In addition, minor variations across experiments explored auxiliary hypotheses. We used longer self-presentations in the dating and professional-context experiments to test whether the length of self-presentations limited participants' accuracy. To keep the three main experiments’ duration comparable, we reduced the number of rated self-presentations to 12 in these two experiments. To explore the effect of increased effort (23), we offered half of the participants in the dating context a bonus payment if they rated at least 75% of the self-presentations correctly. There was no difference in performance between the bonus and no-bonus groups. Finally, to test whether participants could learn to detect generated self-presentations if they received feedback (18), half of the participants in the professional context were told whether their choice was correct after every rating, again with no difference in outcomes. An overview of the experimental designs is included in SI Appendix."



**Modifications**

The authors note that they used 2,000 participants for the original experiment with the hospitality context. They note that a sample size of 1,000 is good enough for replication at end of PNAS report.

Each participant reads 12 self-descriptions, half AI-generated, and half-human-written, sampled from an *anonymized* set of profile bios and paired.

Will not include the incentivization procedure 

### Power Analysis

Given there is no reported effect size, this will require a different estimate of effect. The authors don't actuall report which analyses they conducted to statistically compare means in different experimental conditions. 

### Planned Sample

According to the paper, we're aiming for close to 1,000 participants. 

### Materials

The stimuli for all three contexts have been retrieved from the authors. There are thousands of profiles, 50% AI-generated and 50% human written. I will use embedded data in qualtrics to create a survey that with a significant subset of the data as input of all 2,000 stimuli per context will take an estimated 15 hours of work to complete in of itself.


### Procedure	

Participants will be presented with one of three AI-detection scenarios (dating, business, hospitality). They will then be asked to review the anonymized bios and make a judgement as to whether the bio is generated by a human or an AI. This will be repeated for 12 trials with each participant, with 6 of the profiles generated by humans and 6 generated by chatGPT. 

**For replication**

### Analysis Plan

Can also quote directly, though it is less often spelled out effectively for an analysis strategy section.  The key is to report an analysis strategy that is as close to the original - data cleaning rules, data exclusion rules, covariates, etc. - as possible.  

**Key analysis: two-tailed, two-sample t(z)-test. (Potential Chi-squqred test of distrobutions)**

The means of accuracy scores and distributions between human/ai factor levels will be analyzed. 

With accuracy data collected through Prolific/Qualtrics: 

1) Run python script to anonymize data stimuli and data 

2) Filter for each scenario's human-generated, ai-generated response ratings. 

3) Run two-sample, two-tailed t(z?, chi-sq homogeneity?)-tests for each accuracy type and each treatment. 

Data cleaning rules: this should be straightforward. This should be analyzed with TaT. It's likely that all participants that start the study will finish it. No identifying data is required. 


### Differences from Original Study

The main differences to the original study are that there will be a smalller sample size of particiapnts (reccommended by original authors). Additionally, the stimuli will be subsetted such that there won't be thousands of possible profiles to pull from. Finally, I will be testing only one of the contexts mentioned in the original paper. 

### Methods Addendum (Post Data Collection)

You can comment this section out prior to final report with data collection.

#### Actual Sample
  Sample size, demographics, data exclusions based on rules spelled out in analysis plan

#### Differences from pre-data collection methods plan
  Any differences from what was described as the original plan, or “none”.


## Results


### Data preparation

Data preparation following the analysis plan.

>Below is for the pilot data collection 

**Load Libraries**
```{r include=F}
library(dplyr)
library(tidyverse)
library(tidyr)
library(lme4)

```

**Import Data** 
```{r include=F}

pilota <- read.csv("pilot_a.csv")

```

**Data Exclusion/Wrangling**
```{r include=F}

##Filtering cols that don't have data

pilota<- pilota %>% select(AI_Q1:HU_Q6)

pilota <- pilota[!apply(pilota == "", 1, all),] 

#some rows don't have complete data (these were me testing survey)
pilota <- pilota %>% filter(row_number() !=6)

pilota <- pilota %>% filter(row_number() !=c(1,2))

head(pilota)


```

**Data Tidying**
>for pilot b this will be different

```{r include=F}
#Need to convert data from raw responses to accuracy (0/1 binary variable)

#AI cols
pilota$AI_Q1 <- ifelse(pilota$AI_Q1 == 2, 0, pilota$AI_Q1)

pilota$AI_Q2 <- ifelse(pilota$AI_Q2 == 2, 0, pilota$AI_Q2)

pilota$AI_Q3 <- ifelse(pilota$AI_Q3 == 1, 0, ifelse(pilota$AI_Q3 == 2, 1, pilota$AI_Q3))

pilota$AI_Q4 <- ifelse(pilota$AI_Q4 == 2, 0, pilota$AI_Q4)

pilota$AI_Q5 <- ifelse(pilota$AI_Q5 == 1, 0, ifelse(pilota$AI_Q5 == 2, 1, pilota$AI_Q5))

pilota$AI_Q6 <- ifelse(pilota$AI_Q6 == 1, 0, ifelse(pilota$AI_Q6 == 2, 1, pilota$AI_Q6))

#Human cols 

pilota$HU_Q1 <- ifelse(pilota$HU_Q1 == 2, 0, pilota$HU_Q1)

pilota$HU_Q2 <- ifelse(pilota$HU_Q2 == 2, 0, pilota$HU_Q2)

##NOTE: Weird qualtrics coding had AI selection as four
pilota$HU_Q3 <- ifelse(pilota$HU_Q3 == 4, 0, pilota$HU_Q3)

pilota$HU_Q4 <- ifelse(pilota$HU_Q4 == 1, 0, ifelse(pilota$HU_Q4 == 2, 1, pilota$HU_Q4))

pilota$HU_Q5 <- ifelse(pilota$HU_Q5 == 1, 0, ifelse(pilota$HU_Q5 == 2, 1, pilota$HU_Q5))

pilota$HU_Q6 <- ifelse(pilota$HU_Q6 == 1, 0, ifelse(pilota$HU_Q6 == 2, 1, pilota$HU_Q6))

#Now, we wrangle

#make df with only ai_qs in long form 
#ai_cols <- grep("^AI_Q[1-6]$", names(pilota), value = TRUE)
#pilota_ai <- pilota %>%
 # select(all_of(ai_cols)) %>%
  #pivot_longer(cols = all_of(ai_cols), names_to = "AI_Q", values_to = "AI_acc")

#make df with only hu_qs in long form 
#hu_cols <- grep("^HU_Q[1-6]$", names(pilota), value = TRUE)
#pilota_hu <- pilota %>%
  #select(all_of(hu_cols)) %>%
  #pivot_longer(cols = all_of(hu_cols), names_to = "HU_Q", values_to = "HU_acc")

# make a row for participant numbers (6 obs/participant)
#participants <- data.frame(Participant = rep(1:nrow(pilota), each = 6))

# Combine AI, HU, and participant data frames
#pilota_long <- cbind(participants, pilota_ai, pilota_hu)

#head(pilota_long)
```


**Visualization** See commented-out code above for this plot.

```{r}
#for some reason, the accuracy data is being stored as a factor so I changed it to numeric 
pilota_long$AI_acc <- as.numeric(pilota_long$AI_acc)
pilota_long$HU_acc <- as.numeric(pilota_long$HU_acc)


mean_accuracy <- pilota_long %>%
  summarize(Mean_AI_acc = mean(AI_acc), 
            Mean_HU_acc = mean(HU_acc))

# Gather the data into long format for plotting
mean_accuracy_long <- mean_accuracy %>%
  gather(key = "Condition", value = "Mean_Accuracy", Mean_AI_acc, Mean_HU_acc)

# Create a bar plot with error bars
ggplot(mean_accuracy_long, aes(x = Condition, y = Mean_Accuracy, fill = Condition)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8), width = 0.7) +
  geom_errorbar(aes(ymin = Mean_Accuracy - sd(Mean_Accuracy) / sqrt(length(Mean_Accuracy)), 
                    ymax = Mean_Accuracy + sd(Mean_Accuracy) / sqrt(length(Mean_Accuracy))), 
                position = position_dodge(width = 0.8), width = 0.25) +
  labs(x = "Condition", y = "Mean Accuracy") +
  theme_minimal() + 
    geom_text(aes(label = round(Mean_Accuracy, 2), x = Condition), 
              position = position_dodge(width = 0.8), 
              vjust = -0.5, hjust = 2)

```
**Analysis**

##Start here only with long form data with condition cols and _accs 
```{r}
# Calculate overall accuracy for both AI and HU conditions (compare raw means)
overall_accuracy <- mean(c(pilota_long$AI_acc, pilota_long$HU_acc))

# Print the overall accuracy
print(overall_accuracy)


# Calculate chance accuracy (50%)
chance_accuracy <- 0.5

# Compare overall accuracy to chance accuracy
accuracy_comparison <- data.frame(
  Measure = c("Overall Accuracy", "Chance Accuracy"),
  Accuracy = c(overall_accuracy, chance_accuracy)
)

# Print the comparison
print(accuracy_comparison)


#Binomial Test

# Observed accuracy
observed_accuracy <- mean(c(pilota_long$AI_acc, pilota_long$HU_acc))

# Expected (chance) accuracy
chance_accuracy <- 0.5

# Perform a one-sample binomial test
binomial_test <- binom.test(sum(c(pilota_long$AI_acc, pilota_long$HU_acc)), 
                            length(c(pilota_long$AI_acc, pilota_long$HU_acc)), 
                            p = chance_accuracy, alternative = "two.sided")

# Print the results of the binomial test
print(binomial_test)
```


##Start here only with pilot a with AI_Qs and HU_Qs coded for 0-1 acc.
```{r}
pilota_lr <- pilota %>%
  pivot_longer(cols = starts_with("AI_Q") | starts_with("HU_Q"),
               names_to = "Question",
               values_to = "Accuracy")

# Create a binary 'Condition' variable (0 for AI, 1 for HU)
pilota_lr$Condition <- ifelse(grepl("^AI_", pilota_lr$Question), 0, 1)

# Create a unique participant identifier column
pilota_lr$Participant <- rep(1:(nrow(pilota_lr)/12), each = 12)

# Add 'Participant' as a factor variable
pilota_lr$Participant <- as.factor(pilota_lr$Participant)

# Convert 'Accuracy' to a numeric variable if it's not already
pilota_lr$Accuracy <- as.numeric(pilota_lr$Accuracy)

# Fit a logistic regression model controlling for nested observations
logistic_model <- glmer(Accuracy ~ Condition + (1 | Participant), 
                        data = pilota_lr, family = binomial)

# Summary of the logistic regression model
summary(logistic_model)
```

### Doing a t-test comparison 
```{r}

individual_avg <- pilota_lr %>%
  group_by(Participant, Condition) %>%
  summarize(Avg_Accuracy = mean(Accuracy))

str(individual_avg)

t_test_result <- t.test(Avg_Accuracy ~ Condition, data = individual_avg)

# Display the t-test result
t_test_result

```


```{r}
### Confirmatory analysis

The analyses as specified in the analysis plan.  

*Side-by-side graph with original graph is ideal here*

### Exploratory analyses

Any follow-up analyses desired (not required).  

## Discussion

### Summary of Replication Attempt

Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.  

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt.  None of these need to be long.